# -*- coding: utf-8 -*-
"""Imersão Agentes de IA - Alura + Google Gemini.ipynb - Aula 01

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NiBLK4-FUC6D_B8V5CN6bUXPnCC11wmz

# Aula 01
"""

# -----------------------------------------------------------------------------
# Instalação das dependências iniciais (bibliotecas principais do LangChain e Gemini)
# -----------------------------------------------------------------------------
# !pip install -q --upgrade langchain langchain-google-genai google-generativeai

"""Importação da API Key"""

# Biblioteca do Google Colab para recuperar variáveis de ambiente seguras
from langchain_google_genai import ChatGoogleGenerativeAI

# Recupera a API Key do Gemini armazenada no ambiente do Colab
import os
from dotenv import load_dotenv

# Carrega variáveis do .env
load_dotenv()
GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")
"""Conexão com o Gemini"""

# Cria o objeto LLM (modelo de linguagem) com as configurações do Gemini
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",  # Modelo de IA escolhido
    temperature=0.0,           # Controle da aleatoriedade (0.0 = respostas determinísticas)
    api_key=GOOGLE_API_KEY     # Chave de autenticação
)

# Faz um teste simples para verificar se a conexão está funcionando
resp_test = llm.invoke("Quem é você? Seja criativo.")
print(resp_test.content)

# -----------------------------------------------------------------------------
# Prompt de TRIAGEM: define como o agente deve classificar pedidos dos usuários
# -----------------------------------------------------------------------------
TRIAGEM_PROMPT = (
    "Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. "
    "Dada a mensagem do usuário, retorne SOMENTE um JSON com:\n"
    "{\n"
    '  "decisao": "AUTO_RESOLVER" | "PEDIR_INFO" | "ABRIR_CHAMADO",\n'
    '  "urgencia": "BAIXA" | "MEDIA" | "ALTA",\n'
    '  "campos_faltantes": ["..."]\n'
    "}\n"
    "Regras:\n"
    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas.\n'
    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema.\n'
    '- **ABRIR_CHAMADO**: Pedidos de exceção, aprovação, acesso especial ou solicitações explícitas de abrir chamado.'
    "Analise a mensagem e decida a ação mais apropriada."
)

# -----------------------------------------------------------------------------
# Definição do modelo de saída estruturada com Pydantic
# -----------------------------------------------------------------------------
from pydantic import BaseModel, Field
from typing import Literal, List, Dict

class TriagemOut(BaseModel):
    decisao: Literal["AUTO_RESOLVER", "PEDIR_INFO", "ABRIR_CHAMADO"]
    urgencia: Literal["BAIXA", "MEDIA", "ALTA"]
    campos_faltantes: List[str] = Field(default_factory=list)

# Cria um LLM dedicado à triagem
llm_triagem = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.0,
    api_key=GOOGLE_API_KEY
)

# -----------------------------------------------------------------------------
# Montagem da cadeia de triagem
# -----------------------------------------------------------------------------
from langchain_core.messages import SystemMessage, HumanMessage

# Define a chain que retorna a saída diretamente no formato TriagemOut
triagem_chain = llm_triagem.with_structured_output(TriagemOut)

# Função utilitária de triagem
def triagem(mensagem: str) -> Dict:
    saida: TriagemOut = triagem_chain.invoke([
        SystemMessage(content=TRIAGEM_PROMPT),
        HumanMessage(content=mensagem)
    ])
    return saida.model_dump()

# -----------------------------------------------------------------------------
# Testes iniciais da triagem
# -----------------------------------------------------------------------------
testes = [
    "Posso reembolsar a internet?",
    "Quero mais 5 dias de trabalho remoto. Como faço?",
    "Posso reembolsar cursos ou treinamentos da Alura?",
    "Quantas capivaras tem no Rio Pinheiros?"
]

for msg_teste in testes:
    print(f"Pergunta: {msg_teste}\n -> Resposta: {triagem(msg_teste)}\n")

# -----------------------------------------------------------------------------
# Instalação das dependências adicionais para RAG
# -----------------------------------------------------------------------------
# !pip install -q --upgrade langchain_community faiss-cpu langchain-text-splitters pymupdf

# -----------------------------------------------------------------------------
# Carregando documentos PDF
# -----------------------------------------------------------------------------
from pathlib import Path
from langchain_community.document_loaders import PyMuPDFLoader

docs: list[str] = []

# Procura arquivos PDF no diretório do Colab (/content/)
pdf_path = Path(__file__).parent / "pdf"
for n in pdf_path.glob("*.pdf"):
    try:
        loader = PyMuPDFLoader(str(n))
        docs.extend(loader.load())
        print(f"Carregado com sucesso arquivo {n.name}")
    except Exception as e:
        print(f"Erro ao carregar arquivo {n.name}: {e}")

print(f"Total de documentos carregados: {len(docs)}")

# -----------------------------------------------------------------------------
# Quebrando os documentos em chunks (pedaços menores)
# -----------------------------------------------------------------------------
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
chunks = splitter.split_documents(docs)

# Mostra os chunks criados
for chunk in chunks:
    print(f"Chunk {chunk}")
    print("-"*50)

# -----------------------------------------------------------------------------
# Criando Embeddings (representação vetorial do texto)
# -----------------------------------------------------------------------------
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001",
    google_api_key=GOOGLE_API_KEY
)

# -----------------------------------------------------------------------------
# Criando o Vector Store (FAISS) para buscas semânticas
# -----------------------------------------------------------------------------
from langchain_community.vectorstores import FAISS

vectorStore = FAISS.from_documents(chunks, embeddings)

# -----------------------------------------------------------------------------
# Configurando o Retriever (mecanismo de busca nos embeddings)
# -----------------------------------------------------------------------------
retriever = vectorStore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.3, "k": 4}  # busca até 4 chunks relevantes
)

# -----------------------------------------------------------------------------
# Criando o Prompt para RAG
# -----------------------------------------------------------------------------
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

prompt_rag = ChatPromptTemplate.from_messages([
    ("system",
     "Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. "
     "Responda SOMENTE com base no contexto fornecido. "
     "Se não houver base suficiente, responda apenas 'Não sei'."),
    ("human", "Pergunta: {input}\n\nContexto:\n{context}")
])

# Cadeia que combina documentos com o modelo de linguagem
document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)

# -----------------------------------------------------------------------------
# Funções utilitárias para manipular textos e formatar citações
# -----------------------------------------------------------------------------
import re, pathlib
from typing import List, Dict

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()

def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:
    txt = _clean_text(texto)
    termos = [t.lower() for t in re.findall(r"\w+", query or "") if len(t) >= 4]
    pos = -1
    for t in termos:
        pos = txt.lower().find(t)
        if pos != -1: break
    if pos == -1: pos = 0
    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)
    return txt[ini:fim]

def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:
    cites, seen = [], set()
    for d in docs_rel:
        src = pathlib.Path(d.metadata.get("source", "")).name
        page = int(d.metadata.get("page", 0)) + 1
        key = (src, page)
        if key in seen:
            continue
        seen.add(key)
        cites.append({
            "documento": src,
            "pagina": page,
            "trecho": extrair_trecho(d.page_content, query)
        })
    return cites[:3]

# -----------------------------------------------------------------------------
# Função principal do RAG
# -----------------------------------------------------------------------------
def realizarPerguntaRAG(pergunta: str) -> Dict:
    docs_relacionados = retriever.invoke(pergunta)

    if not docs_relacionados:
        return {"answer": "Não sei.",
                "citacoes": [],
                "contexto_encontrado": False}

    answer = document_chain.invoke({
        "input": pergunta,
        "context": docs_relacionados
    })

    txt = (answer or "").strip()

    if txt.rstrip(".!?") == "Não sei":
        return {"answer": "Não sei.",
                "citacoes": [],
                "contexto_encontrado": False}

    return {"answer": txt,
            "citacoes": formatar_citacoes(docs_relacionados, pergunta),
            "contexto_encontrado": True}

# -----------------------------------------------------------------------------
# Testando o RAG
# -----------------------------------------------------------------------------
testes = [
    "Posso reembolsar a internet?",
    "Quero mais 5 dias de trabalho remoto. Como faço?",
    "Posso reembolsar cursos ou treinamentos da Alura?",
    "Quantas capivaras tem no Rio Pinheiros?"
]

for msg_teste in testes:
    resposta = realizarPerguntaRAG(msg_teste)
    print(f"\nPERGUNTA: {msg_teste}")
    print(f"RESPOSTA: {resposta['answer']}")
    if resposta["contexto_encontrado"]:
        print("CITAÇÕES:")
        for c in resposta["citacoes"]:
            print(f" - Documento: {c['documento']}, Página: {c['pagina']}")
            print(f"   Trecho: {c['trecho']}")
        print("------------------------------------")

# -----------------------------------------------------------------------------
# Definição do estado do agente (memória compartilhada entre nós)
# -----------------------------------------------------------------------------
from typing import TypedDict, Optional, List, Dict
from langchain_core.messages import SystemMessage, HumanMessage

class AgentState(TypedDict, total=False):
    pergunta: str
    triagem: dict
    resposta: Optional[str]
    citacoes: List[dict]
    rag_sucesso: bool
    acao_final: str

# -----------------------------------------------------------------------------
# Função de triagem encapsulada
# -----------------------------------------------------------------------------
def executar_triagem(mensagem: str) -> Dict:
    saida: TriagemOut = triagem_chain.invoke([
        SystemMessage(content=TRIAGEM_PROMPT),
        HumanMessage(content=mensagem)
    ])
    return saida.model_dump()

# -----------------------------------------------------------------------------
# Definição dos nós do grafo (ações do agente)
# -----------------------------------------------------------------------------
def nodeTriagem(state: AgentState) -> AgentState:
    print("Executando nó de triagem...")
    return {"triagem": executar_triagem(state["pergunta"])}

def nodeAutoResolver(state: AgentState) -> AgentState:
    print("Executando o nó auto_resolver...")
    resposta_rag = realizarPerguntaRAG(state["pergunta"])
    update: AgentState = {
        "resposta": resposta_rag["answer"],
        "citacoes": resposta_rag.get("citacoes", []),
        "rag_sucesso": resposta_rag["contexto_encontrado"],
    }
    if resposta_rag["contexto_encontrado"]:
        update["acao_final"] = "AUTO_RESOLVER"
    return update

def nodePedirInformacao(state: AgentState) -> AgentState:
    print("Executando o nó pedir_info...")
    faltantes = state["triagem"].get("campos_faltantes", [])
    detalhe = ",".join(faltantes) if faltantes else "Tem o contexto específico"
    return {
        "resposta": f"Para avançar, preciso que detalhe: {detalhe}",
        "citacoes": [],
        "acao_final": "PEDIR_INFO"
    }

def nodeAbrirChamado(state: AgentState) -> AgentState:
    print("Executando nó de abrir_chamado...")
    triagem = state["triagem"]
    return {
        "resposta": f"Abrindo chamado com urgência {triagem['urgencia']}. "
                    f"Descrição: {state['pergunta'][:140]}",
        "citacoes": [],
        "acao_final": "ABRIR_CHAMADO"
    }

# -----------------------------------------------------------------------------
# Funções de decisão (controlam para onde o fluxo vai)
# -----------------------------------------------------------------------------
KEYWORDS_ABRIR_TICKET = [
    "aprovação", "exceção", "liberação",
    "abrir ticket", "abrir chamado", "acesso especial"
]

def decidirPrincipalDirecao(state: AgentState) -> str:
    print("Decidindo após a triagem...")
    decisao = state["triagem"]["decisao"]
    if decisao == "AUTO_RESOLVER": return "auto"
    if decisao == "PEDIR_INFO": return "info"
    if decisao == "ABRIR_CHAMADO": return "chamado"

def decidirPosAutoResolver(state: AgentState) -> str:
    print("Decidindo após a autoResolver...")
    if state.get("rag_sucesso"):
        print("RAG com sucesso, finalizando o fluxo.")
        return "ok"
    state_pergunta = (state["pergunta"] or "").lower()
    if any(k in state_pergunta for k in KEYWORDS_ABRIR_TICKET):
        print("RAG com falha, mas foi encontrada KEYWORD.")
        return "chamado"
    print("RAG falhou, sem keywords, vou pedir mais informações...")
    return "info"

# -----------------------------------------------------------------------------
# Montagem do grafo de estados (fluxo do agente)
# -----------------------------------------------------------------------------
from langgraph.graph import StateGraph, START, END

workflow = StateGraph(AgentState)

# Adiciona os nós (ações possíveis)
workflow.add_node("triagem", nodeTriagem)
workflow.add_node("auto_resolver", nodeAutoResolver)
workflow.add_node("pedir_info", nodePedirInformacao)
workflow.add_node("abrir_chamado", nodeAbrirChamado)

# Ligações entre os nós (arestas do grafo)
workflow.add_edge(START, "triagem")
workflow.add_conditional_edges("triagem", decidirPrincipalDirecao, {
    "auto": "auto_resolver",
    "info": "pedir_info",
    "chamado": "abrir_chamado"
})
workflow.add_conditional_edges("auto_resolver", decidirPosAutoResolver, {
    "info": "pedir_info",
    "chamado": "abrir_chamado",
    "ok": END
})
workflow.add_edge("pedir_info", END)
workflow.add_edge("abrir_chamado", END)

# Compila o grafo para execução
grafo = workflow.compile()

# -----------------------------------------------------------------------------
# Testes finais do agente completo
# -----------------------------------------------------------------------------
testes = [
    "Posso reembolsar a internet?",
    "Quero mais 5 dias de trabalho remoto. Como faço?",
    "Posso reembolsar cursos ou treinamentos da Alura?",
    "É possível reembolsar certificações do Google Cloud?",
    "Posso obter o Google Gemini de graça?",
    "Qual é a palavra-chave da aula de hoje?",
    "Quantas capivaras tem no Rio Pinheiros?"
]

for msg_test in testes:
    try:
        resposta_final = grafo.invoke({"pergunta": msg_test})
        triag = resposta_final.get("triagem", {})
        print(f"PERGUNTA: {msg_test}")
        print(f"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}")
        print(f"RESPOSTA: {resposta_final.get('resposta')}")
        if resposta_final.get("citacoes"):
            print("CITAÇÕES:")
            for citacao in resposta_final.get("citacoes"):
                print(f" - Documento: {citacao['documento']}, Página: {citacao['pagina']}")
                print(f"   Trecho: {citacao['trecho']}")
        print("------------------------------------")
    except Exception as e:
        print(f"PERGUNTA: {msg_test}")
        print("Ocorreu um erro ao processar a pergunta (provavelmente cota do Gemini excedida).")
        print("------------------------------------")
